{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent.parent))\n",
    "\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.utils.utils_fn import capture_variables, gather_variable_info\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>condition</th>\n",
       "      <th>state</th>\n",
       "      <th>city</th>\n",
       "      <th>local_pickup</th>\n",
       "      <th>free_shipping</th>\n",
       "      <th>shipping_mode</th>\n",
       "      <th>listing_type</th>\n",
       "      <th>buying_mode</th>\n",
       "      <th>attribute_group_id</th>\n",
       "      <th>attribute_group</th>\n",
       "      <th>...</th>\n",
       "      <th>status</th>\n",
       "      <th>accepts_mercadopago</th>\n",
       "      <th>currency</th>\n",
       "      <th>automatic_relist</th>\n",
       "      <th>title</th>\n",
       "      <th>stock_quantity</th>\n",
       "      <th>available_quantity</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>date_difference_hr</th>\n",
       "      <th>time_difference_hr</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mla5501620002</th>\n",
       "      <td>used</td>\n",
       "      <td>capital federal</td>\n",
       "      <td>nuñez</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>not_specified</td>\n",
       "      <td>bronze</td>\n",
       "      <td>buy_it_now</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>active</td>\n",
       "      <td>True</td>\n",
       "      <td>ars</td>\n",
       "      <td>False</td>\n",
       "      <td>timbre inahalambrico</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>1440.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mla2357269269</th>\n",
       "      <td>used</td>\n",
       "      <td>buenos aires</td>\n",
       "      <td>avellaneda</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>not_specified</td>\n",
       "      <td>bronze</td>\n",
       "      <td>buy_it_now</td>\n",
       "      <td>dflt</td>\n",
       "      <td>otros</td>\n",
       "      <td>...</td>\n",
       "      <td>active</td>\n",
       "      <td>True</td>\n",
       "      <td>ars</td>\n",
       "      <td>False</td>\n",
       "      <td>lote de 2 cinturones. 1 nuevo con etiqueta.mic...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>695.485278</td>\n",
       "      <td>1440.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mla4505955642</th>\n",
       "      <td>used</td>\n",
       "      <td>buenos aires</td>\n",
       "      <td>acassuso</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>me2</td>\n",
       "      <td>bronze</td>\n",
       "      <td>buy_it_now</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>active</td>\n",
       "      <td>True</td>\n",
       "      <td>ars</td>\n",
       "      <td>False</td>\n",
       "      <td>revista instituto de historia del derecho rica...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>1440.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mla7853937105</th>\n",
       "      <td>used</td>\n",
       "      <td>capital federal</td>\n",
       "      <td>retiro</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>not_specified</td>\n",
       "      <td>free</td>\n",
       "      <td>buy_it_now</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>active</td>\n",
       "      <td>True</td>\n",
       "      <td>ars</td>\n",
       "      <td>False</td>\n",
       "      <td>susan sontag - la enfermedad y sus metaforas -...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>1440.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mla7813601724</th>\n",
       "      <td>new</td>\n",
       "      <td>capital federal</td>\n",
       "      <td>almagro</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>not_specified</td>\n",
       "      <td>silver</td>\n",
       "      <td>buy_it_now</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>active</td>\n",
       "      <td>True</td>\n",
       "      <td>ars</td>\n",
       "      <td>False</td>\n",
       "      <td>vendas cambric marca vendsur de 10cm x 3mt en ...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>1440.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              condition            state        city  local_pickup  \\\n",
       "product_id                                                           \n",
       "mla5501620002      used  capital federal       nuñez          True   \n",
       "mla2357269269      used     buenos aires  avellaneda          True   \n",
       "mla4505955642      used     buenos aires    acassuso          True   \n",
       "mla7853937105      used  capital federal      retiro          True   \n",
       "mla7813601724       new  capital federal     almagro          True   \n",
       "\n",
       "               free_shipping  shipping_mode listing_type buying_mode  \\\n",
       "product_id                                                             \n",
       "mla5501620002          False  not_specified       bronze  buy_it_now   \n",
       "mla2357269269          False  not_specified       bronze  buy_it_now   \n",
       "mla4505955642          False            me2       bronze  buy_it_now   \n",
       "mla7853937105          False  not_specified         free  buy_it_now   \n",
       "mla7813601724          False  not_specified       silver  buy_it_now   \n",
       "\n",
       "              attribute_group_id attribute_group  ...  status  \\\n",
       "product_id                                        ...           \n",
       "mla5501620002               None            None  ...  active   \n",
       "mla2357269269               dflt           otros  ...  active   \n",
       "mla4505955642               None            None  ...  active   \n",
       "mla7853937105               None            None  ...  active   \n",
       "mla7813601724               None            None  ...  active   \n",
       "\n",
       "              accepts_mercadopago  currency automatic_relist  \\\n",
       "product_id                                                     \n",
       "mla5501620002                True       ars            False   \n",
       "mla2357269269                True       ars            False   \n",
       "mla4505955642                True       ars            False   \n",
       "mla7853937105                True       ars            False   \n",
       "mla7813601724                True       ars            False   \n",
       "\n",
       "                                                           title  \\\n",
       "product_id                                                         \n",
       "mla5501620002                               timbre inahalambrico   \n",
       "mla2357269269  lote de 2 cinturones. 1 nuevo con etiqueta.mic...   \n",
       "mla4505955642  revista instituto de historia del derecho rica...   \n",
       "mla7853937105  susan sontag - la enfermedad y sus metaforas -...   \n",
       "mla7813601724  vendas cambric marca vendsur de 10cm x 3mt en ...   \n",
       "\n",
       "              stock_quantity  available_quantity  total_amount  \\\n",
       "product_id                                                       \n",
       "mla5501620002              1                   1           0.0   \n",
       "mla2357269269              8                   8           0.0   \n",
       "mla4505955642              3                   3           0.0   \n",
       "mla7853937105              1                   1           0.0   \n",
       "mla7813601724              7                   7        2010.0   \n",
       "\n",
       "               date_difference_hr  time_difference_hr  \n",
       "product_id                                             \n",
       "mla5501620002            0.000833              1440.0  \n",
       "mla2357269269          695.485278              1440.0  \n",
       "mla4505955642            0.000833              1440.0  \n",
       "mla7853937105            0.000833              1440.0  \n",
       "mla7813601724            0.000556              1440.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definir la ruta absoluta para la carpeta de pipelines\n",
    "root_path = Path.cwd().resolve().parent.parent\n",
    "\n",
    "# Crear el directorio si no existe\n",
    "root_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Lectura del dataset\n",
    "data = pd.read_parquet(\n",
    "    path=str(root_path / 'data/processed/data_processed.parquet'), \n",
    ")\n",
    "\n",
    "# Setear los ids como índices\n",
    "data: pd.DataFrame = data.set_index('product_id')\n",
    "data.sample(5, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separación de los conjuntos de datos\n",
    "# ===================================================================================================================\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Inicializar una semilla\n",
    "SEED = 25\n",
    "\n",
    "# Separamos los features y el target\n",
    "X = data.loc[:, data.columns != 'condition'] # type: ignore\n",
    "y = data.loc[:, data.columns == 'condition'].squeeze() # type: ignore\n",
    "\n",
    "# Verificar que los índices coinciden\n",
    "assert (X.index == y.index).all(), 'Los índices de X e y no coinciden'\n",
    "\n",
    "# Dividir el conjunto original en 70% entrenamiento y 30% para pruebas y validación\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=0.4, \n",
    "    random_state=SEED, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Luego, dividir el 30% restante en 20% para validación y 10% para pruebas\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=1/2, \n",
    "    random_state=SEED, \n",
    "    stratify=y_temp\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Subconjunto   | Proporción | Descripción                                                 |\n",
    "|---------------|------------|-------------------------------------------------------------|\n",
    "| Entrenamiento | 60%        | Se usa para entrenar el modelo.                             |\n",
    "| Validación    | 20%        | Se usa para afinar hiperparámetros y evaluar durante el ajuste. |\n",
    "| Prueba        | 20%        | Se usa para evaluar el rendimiento final del modelo.        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Codificación del target\n",
    "le = LabelEncoder()\n",
    "y_train = pd.Series(le.fit_transform(y_train), index=y_train.index)\n",
    "y_val = pd.Series(le.fit_transform(y_val), index=y_val.index)\n",
    "y_test = pd.Series(le.fit_transform(y_test), index=y_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tTipos de variables\n",
      "Hay 5 variables continuas\n",
      "Hay 0 variables discretas\n",
      "Hay 0 variables temporales\n",
      "Hay 15 variables categóricas\n"
     ]
    }
   ],
   "source": [
    "# Función para capturar los tipos de variables\n",
    "continuous, categoricals, discretes, temporaries = capture_variables(\n",
    "    data=X_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"continuous_more_than_threshold\": [],\n",
      "    \"continuous_less_than_threshold\": [],\n",
      "    \"categoricals_more_than_threshold\": [\n",
      "        \"attribute_group_id\",\n",
      "        \"attribute_group\",\n",
      "        \"attribute_id\"\n",
      "    ],\n",
      "    \"categoricals_less_than_threshold\": [\n",
      "        \"state\",\n",
      "        \"city\"\n",
      "    ],\n",
      "    \"categoricals_high_cardinality\": [\n",
      "        \"state\",\n",
      "        \"city\",\n",
      "        \"listing_type\",\n",
      "        \"attribute_group_id\",\n",
      "        \"attribute_group\",\n",
      "        \"attribute_id\",\n",
      "        \"title\"\n",
      "    ],\n",
      "    \"categoricals_low_cardinality\": [\n",
      "        \"local_pickup\",\n",
      "        \"free_shipping\",\n",
      "        \"shipping_mode\",\n",
      "        \"buying_mode\",\n",
      "        \"status\",\n",
      "        \"accepts_mercadopago\",\n",
      "        \"currency\",\n",
      "        \"automatic_relist\"\n",
      "    ],\n",
      "    \"discretes_more_than_threshold\": [],\n",
      "    \"discretes_less_than_threshold\": [],\n",
      "    \"discretes_high_cardinality\": [],\n",
      "    \"discretes_low_cardinality\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Capturar información de las variables\n",
    "info = gather_variable_info(\n",
    "    X=X_train,\n",
    "    continuous=continuous,\n",
    "    categoricals=categoricals,\n",
    "    discretes=discretes,\n",
    "    missing_threshold=0.05,       \n",
    "    cardinality_threshold=5 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from feature_engine.selection import DropFeatures\n",
    "from feature_engine.selection import DropConstantFeatures\n",
    "from feature_engine.selection import DropCorrelatedFeatures\n",
    "from feature_engine.preprocessing import MatchVariables, MatchCategories\n",
    "\n",
    "high_cardinality = ['title', 'city']\n",
    "categoricals_more_than_threshold = info['categoricals_more_than_threshold']\n",
    "\n",
    "# Pipeline de procesadores\n",
    "pipe = Pipeline([\n",
    "    ('drop-features', DropFeatures(features_to_drop=categoricals_more_than_threshold + high_cardinality)),\n",
    "    ('constant-features', DropConstantFeatures(variables=[var for var in continuous + categoricals if var not in categoricals_more_than_threshold + high_cardinality], \n",
    "                                               missing_values='ignore',\n",
    "                                               tol=0.95)),\n",
    "    ('match-features', MatchVariables(missing_values='ignore')),\n",
    "    ('correlated-features', DropCorrelatedFeatures(variables=continuous,\n",
    "                                                   missing_values='ignore',\n",
    "                                                   method='pearson',\n",
    "                                                   threshold=0.8)),\n",
    "    ('match-categories', MatchCategories(missing_values='ignore')),\n",
    "])\n",
    "\n",
    "pipe.fit(X_train)\n",
    "X_train = pipe.transform(X_train)\n",
    "X_val = pipe.transform(X_val)\n",
    "X_test = pipe.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['local_pickup'] = X_train['local_pickup'].astype('category')\n",
    "X_val['local_pickup'] = X_val['local_pickup'].astype('category')\n",
    "X_test['local_pickup'] = X_test['local_pickup'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tTipos de variables\n",
      "Hay 4 variables continuas\n",
      "Hay 0 variables discretas\n",
      "Hay 0 variables temporales\n",
      "Hay 4 variables categóricas\n"
     ]
    }
   ],
   "source": [
    "# Función para capturar los tipos de variables\n",
    "continuous, categoricals, discretes, temporaries = capture_variables(\n",
    "    data=X_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"continuous_more_than_threshold\": [],\n",
      "    \"continuous_less_than_threshold\": [],\n",
      "    \"categoricals_more_than_threshold\": [],\n",
      "    \"categoricals_less_than_threshold\": [\n",
      "        \"state\"\n",
      "    ],\n",
      "    \"categoricals_high_cardinality\": [\n",
      "        \"state\",\n",
      "        \"listing_type\"\n",
      "    ],\n",
      "    \"categoricals_low_cardinality\": [\n",
      "        \"local_pickup\",\n",
      "        \"shipping_mode\"\n",
      "    ],\n",
      "    \"discretes_more_than_threshold\": [],\n",
      "    \"discretes_less_than_threshold\": [],\n",
      "    \"discretes_high_cardinality\": [],\n",
      "    \"discretes_low_cardinality\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Capturar información de las variables\n",
    "info = gather_variable_info(\n",
    "    X=X_train,\n",
    "    continuous=continuous,\n",
    "    categoricals=categoricals,\n",
    "    discretes=discretes,\n",
    "    missing_threshold=0.05,       \n",
    "    cardinality_threshold=5 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.imputation import RandomSampleImputer, CategoricalImputer\n",
    "from feature_engine.encoding import RareLabelEncoder\n",
    "from feature_engine.discretisation import GeometricWidthDiscretiser\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from feature_engine.encoding import OrdinalEncoder\n",
    "\n",
    "rare_labels_high_cardinality = info['categoricals_high_cardinality']\n",
    "rare_labels_low_cardinality = ['shipping_mode']\n",
    "\n",
    "# Pipeline de procesadores\n",
    "pipe = Pipeline([\n",
    "    ('random-imputer', RandomSampleImputer(\n",
    "        variables=info['categoricals_less_than_threshold'],\n",
    "        random_state=SEED\n",
    "        )),\n",
    "    ('rare-labels-high-cardinality', RareLabelEncoder(\n",
    "        tol=0.05, \n",
    "        n_categories=3, # Probar entre 2, 3, 4 o 5\n",
    "        variables=rare_labels_high_cardinality\n",
    "        )),\n",
    "    ('rare-labels-low-cardinality', RareLabelEncoder(\n",
    "        tol=0.05, \n",
    "        n_categories=2, # Probar entre 2, 3, 4 o 5\n",
    "        variables=rare_labels_low_cardinality\n",
    "        )),\n",
    "    ('discretiser', GeometricWidthDiscretiser(\n",
    "        variables=continuous,\n",
    "        bins=10, # Probar 5 - 10\n",
    "        return_object=True\n",
    "        )),\n",
    "    ('encoder', OrdinalEncoder(\n",
    "        variables=continuous+categoricals,\n",
    "        encoding_method='ordered'\n",
    "        )),\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "X_train = pipe.transform(X_train)\n",
    "X_val = pipe.transform(X_val)\n",
    "X_test = pipe.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "import xgboost as xgb\n",
    "from config.config import settings\n",
    "\n",
    "\n",
    "def eval_performance_metrics(cv_results: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Evalúa los resultados de la validación cruzada y devuelve:\n",
    "      - La iteración óptima (según el mejor test-auc-mean).\n",
    "      - El valor de ROC-AUC en esa iteración.\n",
    "      - El valor de log-loss en esa iteración.\n",
    "\n",
    "    Parámetros:\n",
    "        cv_results (pd.DataFrame): Resultados devueltos por xgb.cv, que incluye columnas como\n",
    "                                   'test-auc-mean' y 'test-logloss-mean'.\n",
    "\n",
    "    Retorna:\n",
    "        best_iteration (int): Número de iteración óptima.\n",
    "        best_roc_auc (float): Valor de ROC-AUC en la iteración óptima.\n",
    "        best_logloss (float): Valor de log-loss en la iteración óptima.\n",
    "    \"\"\"\n",
    "    # Se asume que se desea maximizar el test-auc-mean.\n",
    "    best_iteration = cv_results['test-auc-mean'].idxmax()\n",
    "    best_roc_auc = cv_results.loc[best_iteration, 'test-auc-mean']\n",
    "    best_logloss = cv_results.loc[best_iteration, 'test-logloss-mean']\n",
    "    return best_iteration, best_roc_auc, best_logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para hacer una ejecución\n",
    "def run_experiment(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    X_val: pd.DataFrame,\n",
    "    y_val: pd.Series,\n",
    "    params: dict, \n",
    "    num_boost_round: int, \n",
    "    nfold: int, \n",
    "    early_stopping_rounds: int = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Ejecuta la validación cruzada usando xgb.cv.\n",
    "\n",
    "    Parámetros:\n",
    "        data (dict): Diccionario con las llaves 'X' e 'y' conteniendo los datos de entrada y etiquetas.\n",
    "        params (dict): Parámetros para el modelo XGBoost.\n",
    "        num_boost_round (int): Número máximo de iteraciones (boosting rounds).\n",
    "        nfold (int): Número de folds para la validación cruzada.\n",
    "        early_stopping_rounds (int, opcional): Número de rondas sin mejora para detener anticipadamente.\n",
    "\n",
    "    Retorna:\n",
    "        cv_results (pd.DataFrame): DataFrame con las métricas agregadas en cada iteración.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Combinar train y val para la validación cruzada\n",
    "    X_cv = pd.concat([X_train, X_val], axis=0)\n",
    "    y_cv = pd.concat([y_train, y_val], axis=0)\n",
    "    dtrain_cv = xgb.DMatrix(data=X_cv, label=y_cv)\n",
    "    \n",
    "    cv_results = xgb.cv(\n",
    "        params=params,\n",
    "        dtrain=dtrain_cv,\n",
    "        num_boost_round=num_boost_round,\n",
    "        nfold=nfold,\n",
    "        stratified=True,\n",
    "        early_stopping_rounds=early_stopping_rounds,\n",
    "        metrics=['auc', 'logloss'],\n",
    "        seed=SEED,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    return cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Definir parámetros para XGBoost\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eta': 0.1,\n",
    "        'eval_metric': ['auc', 'logloss'], # Se pueden especificar múltiples métricas; la primera se usará para early stopping\n",
    "        'seed': SEED\n",
    "    }\n",
    "    \n",
    "    num_boost_round = 400\n",
    "    nfold = 10\n",
    "    early_stopping_rounds = 10\n",
    "    experiment_name = 'feature_engineering_experiment'\n",
    "    \n",
    "    # Luego, recrea la carpeta\n",
    "    settings.EXPERIMENTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Iniciar experimento en mlflow\n",
    "    mlflow.set_tracking_uri(uri=settings.MLFLOW_TRACKING_URI)\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    with mlflow.start_run():\n",
    "        \n",
    "        current_run = mlflow.active_run()\n",
    "        print(f'Active run ID: {current_run.info.run_id}') # type: ignore\n",
    "        print(f'Active run name: {current_run.info.run_name}') # type: ignore\n",
    "        \n",
    "        # Registrar parámetros en mlflow (convirtiendo la lista a string para mayor claridad)\n",
    "        mlflow.log_params({\n",
    "            'eval_metric': str(params['eval_metric']),\n",
    "            'num_boost_round': num_boost_round,\n",
    "            'nfold': nfold,\n",
    "            'early_stopping_rounds': early_stopping_rounds\n",
    "        })\n",
    "\n",
    "        # Ejecutar la validación cruzada\n",
    "        cv_results = run_experiment(\n",
    "            X_train=X_train, \n",
    "            y_train=y_train,\n",
    "            X_val=X_val,\n",
    "            y_val=y_val,\n",
    "            params=params,\n",
    "            num_boost_round=num_boost_round,\n",
    "            nfold=nfold,\n",
    "            early_stopping_rounds=early_stopping_rounds\n",
    "        )\n",
    "        print('Resultados de CV:')\n",
    "        print(cv_results)\n",
    "\n",
    "        # Evaluar las métricas obtenidas\n",
    "        best_iteration, best_roc_auc, best_logloss = eval_performance_metrics(cv_results)\n",
    "        print(f'Mejor iteración: {best_iteration}')\n",
    "        print(f'Mejor ROC-AUC: {best_roc_auc}')\n",
    "        print(f'Mejor Log-loss: {best_logloss}')\n",
    "\n",
    "        # Registrar las métricas en mlflow\n",
    "        metrics = {\n",
    "            'best_iteration': best_iteration,\n",
    "            'best_roc_auc': best_roc_auc,\n",
    "            'best_logloss': best_logloss\n",
    "        }\n",
    "        \n",
    "        # Registrar métricas en mlflow\n",
    "        mlflow.log_metrics(metrics)\n",
    "\n",
    "        # Opcional: Entrenar un modelo final con el número óptimo de iteraciones y registrarlo en mlflow\n",
    "        dtrain = xgb.DMatrix(data=X_train, label=y_train)\n",
    "        dval = xgb.DMatrix(X_val, label=y_val)\n",
    "        final_model = xgb.train(\n",
    "            params=params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=best_iteration,\n",
    "            evals=[(dtrain, 'train'), (dval, 'val')]\n",
    "        )\n",
    "        \n",
    "        mlflow.xgboost.log_model(final_model, artifact_path='artifacts/best_model')\n",
    "        mlflow.end_run()\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "### DIOS Y SE PUEDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations  # Para compatibilidad con anotaciones futuras (opcional)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from feature_engine.imputation import RandomSampleImputer, CategoricalImputer\n",
    "from feature_engine.encoding import RareLabelEncoder, OrdinalEncoder\n",
    "from feature_engine.discretisation import GeometricWidthDiscretiser\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.xgboost\n",
    "import concurrent.futures\n",
    "\n",
    "# --- 1. Custom wrapper para KBinsDiscretizer ---\n",
    "class CustomKBinsDiscretiser(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, variables: list[str], n_bins: int = 5, strategy: str = 'uniform') -> None:\n",
    "        self.variables = variables\n",
    "        self.n_bins = n_bins\n",
    "        self.strategy = strategy\n",
    "        self.kbins = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy=strategy)\n",
    "    \n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series | None = None) -> CustomKBinsDiscretiser:\n",
    "        self.kbins.fit(X[self.variables])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        X = X.copy()\n",
    "        X[self.variables] = self.kbins.transform(X[self.variables])\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Función para construir la pipeline de forma dinámica ---\n",
    "def build_pipeline(\n",
    "    dynamic_params: dict[str, dict[str, any]],\n",
    "    info: dict[str, any],\n",
    "    continuous: list[str],\n",
    "    categoricals: list[str],\n",
    "    rare_labels_high_cardinality: list[str],\n",
    "    rare_labels_low_cardinality: list[str],\n",
    "    seed: int\n",
    "    ) -> Pipeline:\n",
    "    \"\"\"\n",
    "    Construye una pipeline de preprocesamiento con pasos:\n",
    "      1. Imputación (RandomSampleImputer o CategoricalImputer)\n",
    "      2. RareLabelEncoder para variables de alta y baja cardinalidad\n",
    "      3. Discretización (CustomKBinsDiscretiser o GeometricWidthDiscretiser)\n",
    "      4. Codificación (OrdinalEncoder)\n",
    "    Los parámetros de cada transformador se pasan de forma dinámica a través de `dynamic_params`.\n",
    "    \"\"\"\n",
    "    # Paso 1: Imputación\n",
    "    imputer_info: dict[str, any] = dynamic_params.get('imputer', {})\n",
    "    imputer_type: str = imputer_info.get('type', 'random')\n",
    "    imputer_params: dict[str, any] = imputer_info.get('params', {})\n",
    "    imputer_params.setdefault('variables', info['categoricals_less_than_threshold'])\n",
    "    if imputer_type == 'random':\n",
    "        imputer: BaseEstimator = RandomSampleImputer(**imputer_params, random_state=seed)\n",
    "    elif imputer_type == \"mode\":\n",
    "        # Para el imputador categórico:\n",
    "        # 1. Se usa imputation_method=\"frequent\" (valor aceptado por la librería)\n",
    "        # 2. Se fuerza a que las variables sean de tipo object, con un transformer que también imprime los dtypes para debug.\n",
    "        from sklearn.preprocessing import FunctionTransformer\n",
    "        from sklearn.pipeline import make_pipeline\n",
    "\n",
    "        # Eliminar cualquier parámetro conflictivo que se haya podido pasar\n",
    "        imputer_params.pop(\"imputation_method\", None)\n",
    "\n",
    "        def cast_columns(X: pd.DataFrame) -> pd.DataFrame:\n",
    "            X = X.copy()\n",
    "            for col in imputer_params.get(\"variables\", []):\n",
    "                # Imprimir los tipos antes y después para ayudar a depurar\n",
    "                print(f\"Columna {col} antes del cast: {X[col].dtype}\")\n",
    "                X[col] = X[col].astype(\"object\")\n",
    "                print(f\"Columna {col} después del cast: {X[col].dtype}\")\n",
    "            return X\n",
    "\n",
    "        cast_transformer = FunctionTransformer(cast_columns)\n",
    "        imputer = make_pipeline(\n",
    "            cast_transformer,\n",
    "            CategoricalImputer(**imputer_params, imputation_method=\"frequent\")\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Tipo de imputador desconocido: {imputer_type}\")\n",
    "    \n",
    "    # Paso 2: Rare Label Encoding\n",
    "    rare_high_info: dict[str, any] = dynamic_params.get('rare_high', {})\n",
    "    rare_high_info.setdefault('tol', 0.05)\n",
    "    rare_high_info.setdefault('variables', rare_labels_high_cardinality)\n",
    "    rare_high_info.setdefault('n_categories', 3)\n",
    "    rare_high: BaseEstimator = RareLabelEncoder(**rare_high_info)\n",
    "    \n",
    "    rare_low_info: dict[str, any] = dynamic_params.get('rare_low', {})\n",
    "    rare_low_info.setdefault('tol', 0.05)\n",
    "    rare_low_info.setdefault('variables', rare_labels_low_cardinality)\n",
    "    rare_low_info.setdefault('n_categories', 2)\n",
    "    rare_low: BaseEstimator = RareLabelEncoder(**rare_low_info)\n",
    "    \n",
    "    # Paso 3: Discretización\n",
    "    discretiser_info: dict[str, any] = dynamic_params.get('discretiser', {})\n",
    "    discretiser_type: str = discretiser_info.get('type', 'kbins')\n",
    "    discretiser_params: dict[str, any] = discretiser_info.get('params', {})\n",
    "    if discretiser_type == 'kbins':\n",
    "        discretiser_params.setdefault('variables', continuous)\n",
    "        discretiser_params.setdefault('n_bins', 5)\n",
    "        discretiser_params.setdefault('strategy', 'uniform')\n",
    "        discretiser: BaseEstimator = CustomKBinsDiscretiser(**discretiser_params)\n",
    "    elif discretiser_type == 'geometric':\n",
    "        discretiser_params.setdefault('variables', continuous)\n",
    "        discretiser_params.setdefault('bins', 5)\n",
    "        discretiser_params.setdefault('return_object', True)\n",
    "        discretiser = GeometricWidthDiscretiser(**discretiser_params)\n",
    "    else:\n",
    "        raise ValueError(f\"Tipo de discretizador desconocido: {discretiser_type}\")\n",
    "    \n",
    "    # Paso 4: Codificación\n",
    "    encoder_info: dict[str, any] = dynamic_params.get('encoder', {}).get('params', {})\n",
    "    encoder_info.setdefault('variables', continuous + categoricals)\n",
    "    encoder_info.setdefault('encoding_method', 'ordered')\n",
    "    encoder: BaseEstimator = OrdinalEncoder(**encoder_info)\n",
    "    \n",
    "    pipe: Pipeline = Pipeline([\n",
    "        ('imputer', imputer),\n",
    "        ('rare_high', rare_high),\n",
    "        ('rare_low', rare_low),\n",
    "        ('discretiser', discretiser),\n",
    "        ('encoder', encoder)\n",
    "    ])\n",
    "    \n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Función run_experiment (para XGBoost cv) con sample_weights ---\n",
    "def run_experiment(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    X_val: pd.DataFrame,\n",
    "    y_val: pd.Series,\n",
    "    xgb_params: dict[str, any],\n",
    "    num_boost_round: int,\n",
    "    nfold: int,\n",
    "    early_stopping_rounds: int,\n",
    "    sample_weights: np.ndarray | None = None\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ejecuta validación cruzada con XGBoost usando la unión de (X_train, X_val) y (y_train, y_val).\n",
    "    Permite pasar sample_weights (o None).\n",
    "    \"\"\"\n",
    "    X_cv: pd.DataFrame = pd.concat([X_train, X_val], axis=0)\n",
    "    y_cv: pd.Series = pd.concat([y_train, y_val], axis=0)\n",
    "    dtrain_cv: xgb.DMatrix = xgb.DMatrix(data=X_cv, label=y_cv, weight=sample_weights)\n",
    "    \n",
    "    cv_results: pd.DataFrame = xgb.cv( # type: ignore\n",
    "        params=xgb_params,\n",
    "        dtrain=dtrain_cv,\n",
    "        num_boost_round=num_boost_round,\n",
    "        nfold=nfold,\n",
    "        stratified=True,\n",
    "        early_stopping_rounds=early_stopping_rounds,\n",
    "        metrics=['auc', 'logloss'],\n",
    "        seed=xgb_params.get('seed', 42),\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    return cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper: función para registrar parámetros con prefijo y \"aplanarlos\"\n",
    "def prefixed_log_params(prefix: str, params: dict[str, any]) -> None:\n",
    "    \"\"\"\n",
    "    Registra en MLflow cada par clave-valor del diccionario, añadiendo un prefijo para que\n",
    "    las claves resultantes sean únicas.\n",
    "    \n",
    "    Si algún valor es un diccionario, se aplana añadiendo el subprefijo correspondiente.\n",
    "    \"\"\"\n",
    "    flattened: dict[str, any] = {}\n",
    "    for key, value in params.items():\n",
    "        if isinstance(value, dict):\n",
    "            for subkey, subvalue in value.items():\n",
    "                flattened[f'{prefix}_{key}_{subkey}'] = subvalue\n",
    "        else:\n",
    "            flattened[f'{prefix}_{key}'] = value\n",
    "    mlflow.log_params(flattened)\n",
    "\n",
    "\n",
    "# --- 4. Función para ejecutar un experimento de pipeline con MLflow ---\n",
    "def run_pipeline_experiment(\n",
    "    dynamic_params: dict[str, dict[str, any]],\n",
    "    xgb_params: dict[str, any],\n",
    "    num_boost_round: int,\n",
    "    nfold: int,\n",
    "    early_stopping_rounds: int,\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    X_val: pd.DataFrame,\n",
    "    y_val: pd.DataFrame,\n",
    "    sample_weights: np.ndarray | None,\n",
    "    info: dict[str, any],\n",
    "    continuous: list[str],\n",
    "    categoricals: list[str],\n",
    "    rare_labels_high_cardinality: list[str],\n",
    "    rare_labels_low_cardinality: list[str],\n",
    "    seed: int\n",
    "    ) -> tuple[float, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Construye y ajusta la pipeline de feature engineering (según parámetros dinámicos), transforma los datos,\n",
    "    y evalúa el rendimiento mediante XGBoost cv.\n",
    "    Se registra el run en MLflow.\n",
    "    \n",
    "    Devuelve una métrica (por ejemplo, best ROC-AUC) y el DataFrame de cv_results.\n",
    "    \"\"\"\n",
    "    with mlflow.start_run() as run:\n",
    "        \n",
    "        # Registrar parámetros dinámicos con prefijos para evitar conflictos en las claves\n",
    "        prefixed_log_params('imputer', dynamic_params.get('imputer', {}))\n",
    "        prefixed_log_params('rare_high', dynamic_params.get('rare_high', {}))\n",
    "        prefixed_log_params('rare_low', dynamic_params.get('rare_low', {}))\n",
    "        prefixed_log_params('discretiser', dynamic_params.get('discretiser', {}))\n",
    "        prefixed_log_params('encoder', dynamic_params.get('encoder', {}))\n",
    "        \n",
    "        mlflow.log_params(xgb_params)\n",
    "        mlflow.log_param('num_boost_round', num_boost_round)\n",
    "        mlflow.log_param('nfold', nfold)\n",
    "        mlflow.log_param('early_stopping_rounds', early_stopping_rounds)\n",
    "        \n",
    "        # Construir la pipeline\n",
    "        pipe: Pipeline = build_pipeline(\n",
    "            dynamic_params, \n",
    "            info, \n",
    "            continuous, \n",
    "            categoricals,\n",
    "            rare_labels_high_cardinality, \n",
    "            rare_labels_low_cardinality, \n",
    "            seed\n",
    "        )\n",
    "        \n",
    "        pipe.fit(X_train, y_train)\n",
    "        X_train_transformed: pd.DataFrame = pipe.transform(X_train)\n",
    "        X_val_transformed: pd.DataFrame = pipe.transform(X_val)\n",
    "        \n",
    "        # Ejecutar XGBoost cv sobre los datos transformados\n",
    "        cv_results: pd.DataFrame = run_experiment(\n",
    "            X_train_transformed, y_train,\n",
    "            X_val_transformed, y_val,\n",
    "            xgb_params, \n",
    "            num_boost_round, \n",
    "            nfold, \n",
    "            early_stopping_rounds,\n",
    "            sample_weights\n",
    "        )\n",
    "        \n",
    "        # Evaluar las métricas\n",
    "        best_iteration, best_roc_auc, best_logloss = eval_performance_metrics(cv_results)\n",
    "        mlflow.log_metric('best_iteration', best_iteration)\n",
    "        mlflow.log_metric('best_roc_auc', best_roc_auc)\n",
    "        mlflow.log_metric('best_logloss', best_logloss)\n",
    "        \n",
    "        # Usamos best_roc_auc como métrica representativa\n",
    "        dummy_metric: float = best_roc_auc\n",
    "        \n",
    "        return dummy_metric, cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Función de evaluación de métricas ---\n",
    "def eval_performance_metrics(cv_results: pd.DataFrame) -> tuple[int, float, float]:\n",
    "    \"\"\"\n",
    "    Dado un DataFrame con los resultados de xgb.cv, retorna:\n",
    "      - best_iteration: índice de la mejor iteración basado en 'test-auc-mean'\n",
    "      - best_roc_auc: valor de 'test-auc-mean' en la mejor iteración\n",
    "      - best_logloss: valor de 'test-logloss-mean' en la mejor iteración\n",
    "    \"\"\"\n",
    "    best_iteration: int = int(cv_results['test-auc-mean'].idxmax())\n",
    "    best_roc_auc: float = float(cv_results.loc[best_iteration, 'test-auc-mean'])\n",
    "    best_logloss: float = float(cv_results.loc[best_iteration, 'test-logloss-mean'])\n",
    "    return best_iteration, best_roc_auc, best_logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Función para ejecutar grid search en paralelo ---\n",
    "def run_grid_search(\n",
    "    grid: list[dict[str, dict[str, any]]],\n",
    "    xgb_params: dict[str, any],\n",
    "    num_boost_round: int,\n",
    "    nfold: int,\n",
    "    early_stopping_rounds: int,\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    X_val: pd.DataFrame,\n",
    "    y_val: pd.Series,\n",
    "    sample_weights: np.ndarray | None,\n",
    "    info: dict[str, any],\n",
    "    continuous: list[str],\n",
    "    categoricals: list[str],\n",
    "    rare_labels_high_cardinality: list[str],\n",
    "    rare_labels_low_cardinality: list[str],\n",
    "    seed: int\n",
    "    ) -> list[tuple[float, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Ejecuta múltiples experimentos en paralelo usando combinaciones de parámetros dinámicos.\n",
    "    Retorna una lista de tuplas (dummy_metric, cv_results).\n",
    "    \"\"\"\n",
    "    results: list[tuple[float, pd.DataFrame]] = []\n",
    "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                run_pipeline_experiment,\n",
    "                dynamic_params, \n",
    "                xgb_params, \n",
    "                num_boost_round, \n",
    "                nfold, \n",
    "                early_stopping_rounds,\n",
    "                X_train, \n",
    "                y_train, \n",
    "                X_val, \n",
    "                y_val, \n",
    "                sample_weights,\n",
    "                info, \n",
    "                continuous, \n",
    "                categoricals,\n",
    "                rare_labels_high_cardinality, \n",
    "                rare_labels_low_cardinality, \n",
    "                seed\n",
    "            )\n",
    "            for dynamic_params in grid\n",
    "        ]\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "            except Exception as e:\n",
    "                print('Error in grid search run:', e)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in grid search run: imputation_method takes only values 'missing' or 'frequent'\n",
      "Error in grid search run: Some of the variables are not categorical. Please cast them as object or categorical before using this transformer.\n",
      "Resultados del grid search:\n"
     ]
    }
   ],
   "source": [
    "def main() -> None:\n",
    "    # Suponemos que ya tienes estas variables definidas en otro lugar:\n",
    "    # X_train, y_train, X_val, y_val, info, continuous, categoricals,\n",
    "    # rare_labels_high_cardinality, rare_labels_low_cardinality, xgb_params\n",
    "\n",
    "    rare_labels_high_cardinality = info['categoricals_high_cardinality']\n",
    "    rare_labels_low_cardinality = ['shipping_mode']\n",
    "\n",
    "    # Configuración de MLflow (asegúrate de que la carpeta mlruns exista y la Tracking URI esté configurada)\n",
    "    from config.config import settings\n",
    "    settings.EXPERIMENTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    mlflow.set_tracking_uri(settings.MLFLOW_TRACKING_URI)\n",
    "    mlflow.set_experiment(\"feature_engineering_experiment\")\n",
    "    \n",
    "    # Definir el grid de parámetros dinámicos para los transformadores  \n",
    "    # (Aquí puedes generar la cuadrícula de combinaciones que desees probar; este es un ejemplo sencillo)\n",
    "    grid = [\n",
    "        {\n",
    "            'imputer': {'type': 'random', 'params': {}},\n",
    "            'rare_high': {'n_categories': 3},\n",
    "            'rare_low': {'n_categories': 2},\n",
    "            'discretiser': {'type': 'kbins', 'params': {'n_bins': 5}},\n",
    "            'encoder': {'params': {}}\n",
    "        },\n",
    "        {\n",
    "            'imputer': {'type': 'mode', 'params': {}},\n",
    "            'rare_high': {'n_categories': 4},\n",
    "            'rare_low': {'n_categories': 3},\n",
    "            'discretiser': {'type': 'geometric', 'params': {'bins': 10}},\n",
    "            'encoder': {'params': {}}\n",
    "        }\n",
    "        # Puedes agregar más combinaciones según tus necesidades\n",
    "    ]\n",
    "    \n",
    "    # Parámetros para XGBoost\n",
    "    xgb_params: dict[str, any] = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eta': 0.1,\n",
    "        'eval_metric': ['auc', 'logloss'],\n",
    "        'seed': 42\n",
    "    }\n",
    "    num_boost_round = 400\n",
    "    nfold = 10\n",
    "    early_stopping_rounds = 10\n",
    "    sample_weights = None  # O un array de pesos si deseas probar esa opción\n",
    "\n",
    "    # Ejecutar la búsqueda de cuadrícula en paralelo\n",
    "    results = run_grid_search(\n",
    "        grid,\n",
    "        xgb_params,\n",
    "        num_boost_round,\n",
    "        nfold,\n",
    "        early_stopping_rounds,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_val,\n",
    "        y_val,\n",
    "        sample_weights,\n",
    "        info,\n",
    "        continuous,\n",
    "        categoricals,\n",
    "        rare_labels_high_cardinality,\n",
    "        rare_labels_low_cardinality,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    # Mostrar los resultados obtenidos (por ejemplo, la mejor ROC-AUC de cada run)\n",
    "    print(\"Resultados del grid search:\")\n",
    "    for best_roc_auc, cv_res in results:\n",
    "        print(f\"Best ROC-AUC: {best_roc_auc}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "state                 category\n",
       "local_pickup          category\n",
       "shipping_mode         category\n",
       "listing_type          category\n",
       "available_quantity       int64\n",
       "total_amount           float64\n",
       "date_difference_hr     float64\n",
       "time_difference_hr     float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "state                 0.000033\n",
       "local_pickup          0.000000\n",
       "shipping_mode         0.000000\n",
       "listing_type          0.000000\n",
       "available_quantity    0.000000\n",
       "total_amount          0.000000\n",
       "date_difference_hr    0.000000\n",
       "time_difference_hr    0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.isnull().mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-meli-u7R8qZeO-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
